{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOiip+M3+FC/RXDL2fuQlg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semaHbo/job-clustering-salary-prediction/blob/main/model_gbt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar -xzf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "FVbutC25Zf6Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"GBT_Model\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "YezQ1Ln0Zp1Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW8OfciVaKm3",
        "outputId": "194f1726-d084-4de5-db55-0ff49abe2fd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8_XbEKebRRn",
        "outputId": "f381a56f-a9f8-4d07-cc99-a5cdb31bcade"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "archive.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = spark.read.parquet(\"/content/drive/MyDrive/datasets/train_df.parquet\")\n",
        "test_df = spark.read.parquet(\"/content/drive/MyDrive/datasets/test_df.parquet\")\n"
      ],
      "metadata": {
        "id": "482PFF-znIw8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Modeli tanımlama\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"Log_Maas_USD\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,  # toplam ağaç sayısı\n",
        "    maxDepth=5,   # ağaçların derinliği\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Eğitimi başlatma\n",
        "gbt_model = gbt.fit(train_df)\n",
        "\n",
        "# Test seti üzerinde tahmin yapma\n",
        "predictions = gbt_model.transform(test_df)\n",
        "\n",
        "# R² skorunu hesaplama\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Log_Maas_USD\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"r2\"\n",
        ")\n",
        "\n",
        "r2_score = evaluator.evaluate(predictions)\n",
        "print(\" GBT R² Skoru:\", round(r2_score, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0SrmAcnoOK5",
        "outputId": "35c74854-e233-4687-a4c5-688e189a6291"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GBT R² Skoru: 0.5453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, PCA\n",
        "from pyspark.ml import Pipeline\n"
      ],
      "metadata": {
        "id": "yX1BDjr_qBoi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer: metni kelimelere ayırmak icin\n",
        "tokenizer = Tokenizer(inputCol=\"Pozisyon\", outputCol=\"pozisyon_token\")\n",
        "\n",
        "# HashingTF: kelimeleri sayısal temsile dönüştürmek icin\n",
        "hashingTF = HashingTF(inputCol=\"pozisyon_token\", outputCol=\"pozisyon_tf\", numFeatures=100)\n",
        "\n",
        "# IDF: kelimelerin önem ağırlıklarını hesaplamak icin\n",
        "idf = IDF(inputCol=\"pozisyon_tf\", outputCol=\"pozisyon_tfidf\")\n",
        "\n",
        "# PCA: Boyutu azaltma (10 bileşene indiriyoruz)\n",
        "pca = PCA(k=10, inputCol=\"pozisyon_tfidf\", outputCol=\"pozisyon_vec\")\n",
        "\n",
        "# Pipeline: tüm adımları zincirlemak icin\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, pca])\n"
      ],
      "metadata": {
        "id": "k-xB8Nl6qjGL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df ve test_df'yi birleştirme dönüşüm tek seferde yapılsın\n",
        "df_all = train_df.union(test_df)\n",
        "\n",
        "# Pipelineı eğitme ve uygulama\n",
        "model = pipeline.fit(df_all)\n",
        "df_transformed = model.transform(df_all)\n"
      ],
      "metadata": {
        "id": "3dwQ5QeZqtpz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"pozisyon_vec\"],  # daha sonra diğer feature'ları da ekleyebiliriz\n",
        "    outputCol=\"features_new\"     # önceki 'features' ile çakışmaması için yeni ad\n",
        ")\n",
        "\n",
        "df_ready = assembler.transform(df_transformed)\n"
      ],
      "metadata": {
        "id": "3W-Wte7Qrn0H"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "df_ready_indexed = df_ready.withColumn(\"row_id\", monotonically_increasing_id())\n"
      ],
      "metadata": {
        "id": "pAo_FXC0r3CG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_indexed = train_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "test_df_indexed = test_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "#setlere indeks ekleme"
      ],
      "metadata": {
        "id": "G--4w9CWuIUD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = df_ready_indexed.select(\"row_id\", \"features_new\")\n"
      ],
      "metadata": {
        "id": "r8BwYidGuMTy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_final = train_df_indexed.join(df_features, on=[\"row_id\"], how=\"inner\")\n",
        "test_df_final = test_df_indexed.join(df_features, on=[\"row_id\"], how=\"inner\")\n",
        "#Merge işlemi: train_df_indexed ve df_features birleştirilir"
      ],
      "metadata": {
        "id": "6WjrihPVuT5h"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_indexed_clean = train_df_indexed.drop(\"Log_Maas_USD\")\n",
        "test_df_indexed_clean = test_df_indexed.drop(\"Log_Maas_USD\")\n",
        "#Hem train_df_indexed hem test_df_indexed'den Log_Maas_USD’yi cikarma"
      ],
      "metadata": {
        "id": "oxJbwRHbvN1C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = df_ready_indexed.select(\"row_id\", \"features_new\", \"Log_Maas_USD\")\n"
      ],
      "metadata": {
        "id": "4PxbrQ3Xvbng"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_final = train_df_indexed_clean.join(df_features, on=\"row_id\", how=\"inner\")\n",
        "test_df_final = test_df_indexed_clean.join(df_features, on=\"row_id\", how=\"inner\")\n"
      ],
      "metadata": {
        "id": "1_CFNOaLvfvK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model egitimi\n",
        "gbt = GBTRegressor(\n",
        "    featuresCol=\"features_new\",\n",
        "    labelCol=\"Log_Maas_USD\",\n",
        "    predictionCol=\"prediction\",\n",
        "    maxIter=100,\n",
        "    maxDepth=5,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "model = gbt.fit(train_df_final)\n",
        "predictions = model.transform(test_df_final)\n",
        "\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"Log_Maas_USD\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"r2\"\n",
        ")\n",
        "\n",
        "r2_score = evaluator.evaluate(predictions)\n",
        "print(\" TF-IDF + PCA GBT R² Skoru (stratified veriyle):\", round(r2_score, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzHAZ040vjNn",
        "outputId": "8c27ed26-1c07-417e-ae6f-df96db49f657"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " TF-IDF + PCA GBT R² Skoru (stratified veriyle): 0.0351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sonuc cok kotu yeni vektör olusturup özellikler dahil edilerek tekrar denenmeli sonuclar analiz edilmeli"
      ],
      "metadata": {
        "id": "zzDj5AtfwDiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[\n",
        "        \"pozisyon_vec\",\n",
        "        \"Deneyim_Seviyesi_Encoded\",\n",
        "        \"Calisma_Tipi_Encoded\",\n",
        "        \"Sirket_Ulke_Encoded\",\n",
        "        \"Sirket_Buyuklugu_Encoded\"\n",
        "    ],\n",
        "    outputCol=\"features_new\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "9Z67GR7Twck5"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}